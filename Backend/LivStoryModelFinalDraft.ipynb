{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LivStoryModelFinalDraft.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evU0LnzaH1NB"
      },
      "source": [
        "#Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZCMgiin_G8Q"
      },
      "source": [
        "import spacy\n",
        "import time\n",
        "import pytextrank\n",
        "import networkx as nx\n",
        "import math\n",
        "import operator\n",
        "import nltk # Used for stopwords and punctuation removal\n",
        "nltk.download()\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 532,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMrCe6oNGvYP",
        "outputId": "9065ba64-b1ce-4298-c0d3-21397a361f96"
      },
      "source": [
        "!python3 -m spacy download en_core_web_sm"
      ],
      "execution_count": 533,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-05 17:49:28.446475: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "Requirement already satisfied: en-core-web-sm==3.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl#egg=en_core_web_sm==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
            "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.7.4.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.5.2)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.4)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (56.0.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
            "Requirement already satisfied: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmiZLt5cG0_0"
      },
      "source": [
        "nlp=spacy.load('en_core_web_sm')"
      ],
      "execution_count": 748,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQWnw8HmT5hA"
      },
      "source": [
        "# Text Rank Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf80wIon51MD"
      },
      "source": [
        "tic=time.time() # We start the time\n",
        "#Input paragraph\n",
        "text=\"\"\"Lion was roaring under the night of full moon bloom.\"\"\"\n",
        "punctuations=\"?:!.,;\"\n",
        "# tokenize everything\n",
        "sentence_words = nltk.word_tokenize(text)\n",
        "str2=''\n",
        "for word in sentence_words:\n",
        "    if word not in punctuations and  word not in stopwords.words('english'):\n",
        "        str2=str2+' '+word\n",
        "# tock=time.time()\n",
        "text=str2\n",
        "#print(str2)\n",
        "# print('\\n',str(tock-tick))"
      ],
      "execution_count": 987,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7U-8xvXDG7o9"
      },
      "source": [
        "# Here we will add the text required on which NLP will take place\n",
        "# text = '''Once upon a time, a lion is roaring.'''"
      ],
      "execution_count": 988,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7YYyql4HGnt"
      },
      "source": [
        "#nlp.add_pipe(\"textrank\") # We add a pipeline at the start of every run"
      ],
      "execution_count": 989,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4Ns5aH8HMgE"
      },
      "source": [
        "doc=nlp(text) #We reinitialise the model everytime\n",
        "#doc._.phrases "
      ],
      "execution_count": 990,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BydyxA5_knLu"
      },
      "source": [
        "**Direct method of solving**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIApsog6HPgs"
      },
      "source": [
        "# for phrase in doc._.phrases:\n",
        "#     print(phrase.text,phrase.rank)\n",
        "#     print(phrase.rank, phrase.count)\n",
        "    #print(phrase.chunks)"
      ],
      "execution_count": 991,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPZHvxDokxiX"
      },
      "source": [
        "**Alternate way of doing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Lnnrz6bRi9_"
      },
      "source": [
        "\n",
        "# from icecream import ic # Importing the icecream library as an alternate for printing\n",
        "\n",
        "# for sent in doc.sents:\n",
        "#     ic(sent.start, sent.end) # This will give the start and stop of sentences \n",
        "# for chunk in doc.noun_chunks:\n",
        "#     ic(chunk.text) #Here we will get all the possible words from the para"
      ],
      "execution_count": 992,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWu55U1hSRoR"
      },
      "source": [
        "#*\n",
        "#Here we will create the graph by making nodes as a tree\n",
        "def increment_edge (graph, node0, node1):\n",
        "    # ic(node0, node1) # Here we print the two nodes of the incoming digraph\n",
        "\n",
        "    if graph.has_edge(node0, node1):\n",
        "        graph[node0][node1][\"weight\"] += 1.0 # Here we form an in-memory graph just like a tree\n",
        "    else:\n",
        "        graph.add_edge(node0, node1, weight=1.0) # If there are no edges then this gets added"
      ],
      "execution_count": 993,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vClsXKMjSV9D"
      },
      "source": [
        "POS_KEPT = [\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\"]\n",
        "# Here we construct the graph using the spacy tags\n",
        "def link_sentence (doc, sent, lemma_graph, seen_lemma):\n",
        "    visited_tokens = []\n",
        "    visited_nodes = []\n",
        "\n",
        "    for i in range(sent.start, sent.end):\n",
        "        token = doc[i] # Here we store each word as a token from the doc\n",
        "\n",
        "        if token.pos_ in POS_KEPT:\n",
        "            key = (token.lemma_, token.pos_) # We then check the word and its properties\n",
        "\n",
        "            if key not in seen_lemma:\n",
        "                seen_lemma[key] = set([token.i])\n",
        "            else:\n",
        "                seen_lemma[key].add(token.i)\n",
        "\n",
        "            node_id = list(seen_lemma.keys()).index(key)\n",
        "\n",
        "            if not node_id in lemma_graph:\n",
        "                lemma_graph.add_node(node_id)\n",
        "\n",
        "            # ic(visited_tokens, visited_nodes)\n",
        "            # ic(list(range(len(visited_tokens) - 1, -1, -1)))\n",
        "\n",
        "            for prev_token in range(len(visited_tokens) - 1, -1, -1):\n",
        "                # ic(prev_token, (token.i - visited_tokens[prev_token]))\n",
        "\n",
        "                if (token.i - visited_tokens[prev_token]) <= 3:\n",
        "                    increment_edge(lemma_graph, node_id, visited_nodes[prev_token])\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            # ic(token.i, token.text, token.lemma_, token.pos_, visited_tokens, visited_nodes)\n",
        "\n",
        "            visited_tokens.append(token.i)\n",
        "            visited_nodes.append(node_id)"
      ],
      "execution_count": 994,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9cue7fMSYzF"
      },
      "source": [
        "lemma_graph = nx.Graph()\n",
        "seen_lemma = {}\n",
        "\n",
        "for sent in doc.sents:\n",
        "    link_sentence(doc, sent, lemma_graph, seen_lemma)\n",
        "    #break # only test one sentence"
      ],
      "execution_count": 995,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7re-MUoeShT8"
      },
      "source": [
        "# ic(seen_lemma)"
      ],
      "execution_count": 996,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBbKtjR-Sm1t"
      },
      "source": [
        "labels = {}\n",
        "keys = list(seen_lemma.keys())\n",
        "\n",
        "for i in range(len(seen_lemma)): # Here we iterate through each sentence to construct their graph\n",
        "    labels[i] = keys[i][0].lower()\n",
        "\n",
        "# labels # Here we check the labels"
      ],
      "execution_count": 997,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JsYc5ADTI_k"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# fig = plt.figure(figsize=(9, 9))\n",
        "# pos = nx.spring_layout(lemma_graph)\n",
        "\n",
        "# nx.draw(lemma_graph, pos=pos, with_labels=False, font_weight=\"bold\")\n",
        "# nx.draw_networkx_labels(lemma_graph, pos, labels);"
      ],
      "execution_count": 998,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zkrb0NLTJFm"
      },
      "source": [
        "ranks = nx.pagerank(lemma_graph) # This variable will store the rank of each node of the graph\n",
        "# ranks"
      ],
      "execution_count": 999,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP6W8I5CTJIS"
      },
      "source": [
        "# for node_id, rank in sorted(ranks.items(), key=lambda x: x[1], reverse=True):\n",
        "#     ic(node_id, rank, labels[node_id])"
      ],
      "execution_count": 1000,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-JplwOsTJLE"
      },
      "source": [
        "#*\n",
        "def collect_phrases (chunk, phrases, counts): # This function will collect all the words and give its rank\n",
        "    chunk_len = chunk.end - chunk.start\n",
        "    sq_sum_rank = 0.0\n",
        "    non_lemma = 0\n",
        "    compound_key = set([])\n",
        "\n",
        "    for i in range(chunk.start, chunk.end):\n",
        "        token = doc[i]\n",
        "        key = (token.lemma_, token.pos_)\n",
        "\n",
        "        if key in seen_lemma:\n",
        "            node_id = list(seen_lemma.keys()).index(key)\n",
        "            rank = ranks[node_id]\n",
        "            sq_sum_rank += rank #depending on its frequency and importance its given a rank\n",
        "            compound_key.add(key)\n",
        "\n",
        "            # ic(token.lemma_, token.pos_, node_id, rank)\n",
        "        else:\n",
        "            non_lemma += 1\n",
        "\n",
        "    # although the noun chunking is greedy, we discount the ranks using a\n",
        "    # point estimate based on the number of non-lemma tokens within a phrase\n",
        "    non_lemma_discount = chunk_len / (chunk_len + (2.0 * non_lemma) + 1.0)\n",
        "\n",
        "    # use root mean square (RMS) to normalize the contributions of all the tokens\n",
        "    phrase_rank = math.sqrt(sq_sum_rank / (chunk_len + non_lemma))\n",
        "    phrase_rank *= non_lemma_discount\n",
        "\n",
        "    # remove spurious punctuation\n",
        "    phrase = chunk.text.lower().replace(\"'\", \"\")\n",
        "\n",
        "    # create a unique key for the the phrase based on its lemma components\n",
        "    compound_key = tuple(sorted(list(compound_key)))\n",
        "\n",
        "    if not compound_key in phrases:\n",
        "        phrases[compound_key] = set([ (phrase, phrase_rank) ])\n",
        "        counts[compound_key] = 1\n",
        "    else:\n",
        "        phrases[compound_key].add( (phrase, phrase_rank) )\n",
        "        counts[compound_key] += 1\n",
        "\n",
        "    # ic(phrase_rank, chunk.text, chunk.start, chunk.end, chunk_len, counts[compound_key])"
      ],
      "execution_count": 1001,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUs-6GQXTJNq"
      },
      "source": [
        "phrases = {}\n",
        "counts = {}\n",
        "\n",
        "for chunk in doc.noun_chunks:\n",
        "    collect_phrases(chunk, phrases, counts) # here we collect all the phrases along with their frequency"
      ],
      "execution_count": 1002,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynk5IQjATJQB"
      },
      "source": [
        "for ent in doc.ents:\n",
        "    collect_phrases(ent, phrases, counts)"
      ],
      "execution_count": 1003,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5wfRrKYTJSX"
      },
      "source": [
        "#*\n",
        "min_phrases = {}\n",
        "\n",
        "for compound_key, rank_tuples in phrases.items():\n",
        "    l = list(rank_tuples)\n",
        "    l.sort(key=operator.itemgetter(1), reverse=True)\n",
        "\n",
        "    phrase, rank = l[0]\n",
        "    count = counts[compound_key]\n",
        "\n",
        "    min_phrases[phrase] = (rank, count)"
      ],
      "execution_count": 1004,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIZJTFYKTenD"
      },
      "source": [
        "# for phrase, (rank, count) in sorted(min_phrases.items(), key=lambda x: x[1][0], reverse=True):\n",
        "#     ic(phrase, count, rank)"
      ],
      "execution_count": 1005,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJfnrqqTTepo",
        "outputId": "c105b35d-2485-4f25-fce2-12bdc85eb608"
      },
      "source": [
        "for node_id, rank in sorted(ranks.items(), key=lambda x: x[1], reverse=True):\n",
        "  print(labels[node_id], rank)\n",
        "# print(labels)"
      ],
      "execution_count": 1006,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "night 0.20388201771934134\n",
            "full 0.2038820177193413\n",
            "moon 0.16643117534112753\n",
            "roar 0.1664311753411275\n",
            "lion 0.12968680693953114\n",
            "bloom 0.1296868069395311\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38UhFBziTesy",
        "outputId": "e3f8a0c8-4598-410a-d370-90748a81ea04"
      },
      "source": [
        "toc=time.time()\n",
        "print(str((toc-tic))+\"s\")"
      ],
      "execution_count": 1007,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.77260422706604s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNZtg3SKoyPk"
      },
      "source": [
        "**Second Alternate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbsum80CHQgH"
      },
      "source": [
        "# from gensim.summarization import keywords\n",
        "# print(keywords(text,words=10,split=True,ratio=0.1))"
      ],
      "execution_count": 1008,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH93cR1t47sW"
      },
      "source": [
        "#Stopwords,Punctuation Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsieENAu49O6",
        "outputId": "f3b94015-ca31-482b-99f2-dc9d6f078efe"
      },
      "source": [
        "#Input paragraph\n",
        "text=\"\"\"Aman is a very good boy. That is one of the reasons why i am writing the whole paragraph.\n",
        " and this is how i will behave!\"\"\"\n",
        "punctuations=\"?:!.,;\"\n",
        "\n",
        "# tokenize everything\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "sentence_words = nltk.word_tokenize(para)\n",
        "\n",
        "#remove punctuations\n",
        "for word in sentence_words:\n",
        "    if word in punctuations:\n",
        "        sentence_words.remove(word)\n",
        "\n",
        "# To see stopwords ki list uncomment\n",
        "from nltk.corpus import stopwords\n",
        "# stopwords.words('english')\n",
        "\n",
        "# Stopwords removal in list\n",
        "# Without stemming\n",
        "final=[]\n",
        "for word in sentence_words:\n",
        "  if word not in stopwords.words('english'):\n",
        "    final.append(word)\n",
        "\n",
        "text=' '.join(final)\n",
        "print(text)"
      ],
      "execution_count": 853,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Aman good boy That one reasons writing whole paragraph behave\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXioNGvW492x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}